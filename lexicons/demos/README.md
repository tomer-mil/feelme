## 1. Description

This folder contains Python code to compute, for a set of speaker utterances and an emotion dimension, the following emotion word usage metrics:

1. Emotion score for each utterance, computed as a mean of the emotion scores of the constituent tokens. 
2. For each utterance, the proportion of tokens that are present in the emotion lexicon. 
3. Utterance Emotion Dynamics (UED) metrics for each speaker.

The two main data files required to run these scripts are:

1. A file with the text of the utterances of each speaker (see **Preparing Your Data** below), formatted as a CSV file..
2. An emotion lexicon file, which associates words with an emotion score along the desired emotion dimensions (see the file `lexicons/NRC-VAD-Lexicon` for an example), also formatted as a CSV file.

Metrics 1 and 2 are computed by a single script, `avgEmoValues.py`. UED metrics are computed by a separate script, more details of which are in the `uedLib` folder. Read on to know more about how these scripts work.

## 2. Requirements
This code was tested with `python3.7` (but should work well with later versions as well). The `requirements.txt` file in this folder lists the external python libraries needed to run various scripts.

## 3. Preparing your Data

### **The Utterances File**

Organize your data as a file of Comma Separated Values (CSV), where each line in the file contains, at minimum, the following information regarding an utterance:
1. A unique identifier for the speaker (for example, the speaker's name)
2. The text of the utterance. This text will be split into a sequence of words (tokens) by the code by separating with a whitespace. 

    **Note 1**: If you want to tokenize your text in another way, do so and re-join the tokens by a whitespace to store in this column. In `clean_data.py`, you can see how we used the [Twokenizer](https://github.com/myleott/ark-twokenize-py) package for our tweets.

    **Note 2**: The code automatically lowercases all tokens while computing metrics.


3. A numerical value indicating the temporal ordering of the utterances. This can be simple integer counter, or a timestamp.

The **first line of this file** must be the header column, which specifies a name for each of the above columns. You will be specifying these column names as configuration paramters for the different Python scripts.

### **The Lexicon file**
This CSV file should have, on each line, a column called `word`, and subsequent columns named by the emotion dimension they represent, which specify a real-valued score of the emotion associated with that word.

See the file `lexicons/NRC-VAD-Lexicon` for an example
## 4. Code Organization
### **Metrics 1 and 2**

The `avgEmoValues.py` file computes metrics 1 and 2 from above for a specified emotion dimension. There are four *command line arguments** that need to be passed:
- **dataPath**: Path to input CSV containing tweets. The first row must be the header, and must contain a column called `text` that contains the text of the text/document.
-  **lexPath**: Path to the lexicon CSV, with first line being the header. The CSV should contain a column called `word` and columns for emotion dimensions that specify emotion score for that word along that dimension (see `lexicons/NRC-VAD-Lexicon.csv` for an example.)
- **lexNames**: The names of the the emotion dimensions that you want to process. Each name should be separated by a space.
- **savePath**: Path to the folder where the output CSV will be stored

The output is a CSV with the following columns for each row (in addition to the original columns):
- `numTokens`: number of tokens in the tweet.
- `numLexTokens`: number of tokens in the tweet that are present in the lexicon
- `lexRatio`: `numLexTokens`/`numTokens`. This is metric 2 listed above.
- `avgLexVal`: emotion score of the tweet. This is metric 1 listed above.

These metrics can then be aggregated at the desired level to obtain emotion word usage scores for a particular speaker, city, country, year, etc.

**Sample Usage**

The `sample_data` folder contains a CSV with snippets of character dialogue from a set of three novels (Pride and Prejudice, Persuasion, The Sign of the Four). Run the following command to see the outputs generated by this script (stored in `sample_data/sample_outputs` folder):

    python avgEmoValues.py --dataPath sample_data/sample_input.csv --lexPath lexicons/NRC-VAD-Lexicon.csv --lexNames valence dominance --savePath sample_data/sample_outputs

### **UED Metrics**

The `uedLib` folder contains code associated with the **Utterance Emotion Dynamics** metrics. 

**How Does UED Work?** 

- For each speaker, the sequence of temporally-ordered utterances is first converted into a sequence of temporally-ordered tokens. 
- The **emotion arc** for the sequence is then computed by taking, starting from the first token, a window of continuous tokens of some length _n_ (we use 20 in our work), and computing the  average of the word--emotion scores in this wndow. (word--emotion scores are obtained using lexicons.)

    **Note**: A more complex model for determining the emotion score of a sequence of tokens can be used here. This will require some modifications to the code in `uedLib/lib/ued.py`. 

- We can compute our UED metrics. The `home base` is calculated as the mean of the window emotion scores, plus or minus one standard deviation on either side. (this is with the default config values, which can be customized according to your needs). 

Check out the [README file](https://github.com/Priya22/TweetDynamics/tree/master/code/uedLib) within the `uedLib` folder for a complete description of the configuration parameters, and how to run the Python script to compute these metrics.









